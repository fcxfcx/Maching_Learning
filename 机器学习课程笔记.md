# 机器学习课程笔记

课程链接：[ML 2022 Spring (ntu.edu.tw)](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)

## 一、机器学习、深度学习的基本概念

什么是机器学习？用一句话来描述，就是让机器拥有去寻找函数的能力。（例如，寻找可以将声音信号识别成文字的函数）我们给出一个输入，通过机器学习来构造一个函数来获取我们需要的输出，下面是不同的函数类型：

**Regression**: The function outputs a scalar （输出一个数值的函数）

**Classification**: Given options (classes), the function outputs the correct one （分类、区分的函数，例如Alpha Go下围棋的过程其实也是Classification，给出棋盘上19*19的选项去选出最佳的一项）

### 1.1 关于预测频道观看人数的例子

能否使用机器学习的方法，根据历史的观看人数，去预测某频道接下来一天的观看人数？如果要做这样一件事，我们可以先去思考这样的一个函数会是什么样子的：

假设$y$是我们想知道的结果（预测的观看人数），$x_1$ 是我们已知的信息（历史数据），可以推测可能的函数构造是$y=b+wx_1$ 其中b和$w$就是我们未知的参数。而如何去获取这些未知的参数就需要一些Domain knowledge，即对于具体问题的一些背景知识理解。

这个带有未知参数的Function，在机器学习中就被称为**Model**（模型），而$x_1$则被称为**Feature**，$w$被称为**Weight**，b则被称为**Bias**。另外一个重要的概念是**Loss**，它是一个函数，接收之前提到的未知参数为输入，并且评估输入的一组参数有多好，可以表示为$L=(b,w)$。这样描述还是略微抽象，可以结合本节的例子更好的理解Loss

### 1.2 如何理解Loss

假设我们知道了一组未知参数（作为Loss的输入），我们就可以构造出一个暂时的Function，而我们拥有历史的真实数据，对每一天的观看人数，我们可以使用构造出的Function去进行运算并得出后一天的预测人数，而对于历史数据来说后一天的真实人数是已知的，因此对于第n天的预测结果我们会得到一个差值$e_n$，当然这个差值不一定是简单的相减，只需要理解为真实值和预测值之间的误差程度，而对于很多天的历史数据而言，我们会计算出很多个差值，Loss也可以表示为$L=\frac{1}{N}\sum_{i=1}^{n}{e_i}$，很好理解的是当L越大说明函数的效果越差。

如果用差值的绝对值去计算误差，得出的Loss会被称为Mean Absolute Error（MAE），如果用差值的平方的方式去计算，则会被称为Mean Square Error（MSE）

MAE和MSE的区别：[机器学习常用损失函数小结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/77686118)



### 1.3  关于Optimization

这一步是机器学习的第三步，但我们已知了如何去评判选出参数的表现的时候，我们的任务就变成了如何选取最佳的参数。这个过程就是Optimization。在这一步里面我们用到的方法是**Gradient Descent**（梯度下降法），它的步骤可以简述为（这里简化了，只考虑了$w$而忽略了b）：

- 随机选取一个初始值$w^0$
- 计算w对Loss的微分是多少：$\frac{\delta L}{\delta w}|_{w=w^0}$ 或者说就是计算在$w^0$处的切线斜率
- 很明显，当斜率为正的时候，减小$w$的值会降低Loss，当斜率为负的时候，增加$w$的值会降低Loss 
- 如何确定$w$的值要更改多少呢？它的表达是$\eta\frac{\delta L}{\delta w}|_{w=w^0}$，这里$\eta$是learning rate，该值需要自己去确定
- 根据$w^1 = w^0 -\eta\frac{\delta L}{\delta w}|_{w=w^0}$更新$w$的值
- 如果当斜率为零的时候，此时$w$的值也不能再改变，这个点称为**Local minima**，但是注意这个点可能并不是Loss最小的点，使得Loss最小的点被称为**Global minima**

这个过程也可以很容易的推广到考虑多个参数的情况，并且计算斜率的过程在大多数的深度学习框架里都包装成了简单的方法。

继续回到之前提到的例子，我们会发现整个训练过程完成后，得出的参数所构造的函数其实表现并不尽人意。这也很好理解，因为我们所假设的函数格式过于简单了，我们只是简单的认为后一天的观看人数和前一天的相关。如果我们考虑更多的数据，去将历史数据更充分的使用，例如使用前七天的观看人数去预测，将函数假设为如下结构：


$$
y= b + \sum_{j=1}^{7}{w_j}{i_j}
$$


我们会发现Loss的值会更小，也就是说训练出来模型的效果更好了。我们甚至可以再进一步，去用更多的历史数据，例如28天、56天甚至更多，也会有更好的模型效果（但是可能到某一个点再继续升高日子并不会提升了）。基于这个思路去假设的函数结构被称为**Linear Model**（线性模型），而这种结果也确实太过简单了。这种由于模型结构过于简单而产生的问题被称为Model Bias，即模型无法去模拟真实的状况。

### 1.4 进一步复杂的Function

前面提到我们使用线性的模型时，无论怎样调整参数，得出的图形也只会是一条直线。而很显然现实情况往往没有这么简单，关于观看人数的曲线呈现一个更复杂的形状（例如多个转折的曲线），我们可以尝试将预测模型更换为一个常数和一组折线的和，而这个折线往往表示为一条S形的曲线，称为Sigmoid Function，它的公式表达是：


$$
y= c \frac{1}{1+e^{-(b+wx_1)}}= c\ sigmoid(b+wx_1)
$$


这样的改变可以理解为，我们带入了更多的Feature，从而减小Model Bias，相应的模型可以表示为：


$$
y=b+\sum_{i}{c_i \ sigmoid(b_i+\sum_{j}{w_{ij}x_j})}
$$


这里的$i$是sigmoid函数的编号，而$j$则是feature的编号（这里的x就是日期），下面将这个模型表示为了图示和更简洁的公式表示，其中右侧的b，W和x是矩阵表示，因为括号内部的累加可以表示为矩阵乘法。而经过sigmoid方程后它们则表现为三条不同的曲线。

![](.\Resource\1.png)

这样的表示也带来了更多的参数，除了x为feature以外，其他都是需要确定的参数，注意这里的W等都是矩阵，因此将它们展开实际上是多个需要确定的参数，我们把这些参数全部列出来，归纳表达为$\theta$，同样我们需要去计算Loss来评价这些参数的表现。

同样的，在选取一个初始值之后，对于每个参数，都需要去与L进行微分计算，这样就得出了一个向量，内部是所有参数对L做微分计算的结果，表示为：$g=\nabla L(\theta^0)$，这里用的是初始点做例子。而在更新过程中，会根据learning rate同时更新所有的参数，即$\theta^0-\eta g \rightarrow \theta^1$。而在实际操作时，往往数据量会非常大，而常见的作法是将其分为多个**batch**，而每次计算Loss的时候使用不同的batch，当使用完一轮batch的时候就称为一个**epoch**。例如有一万条数据，而一个Batch的数量是10，则有1000个batch，那么每一个epoch内就更新了1000次参数。

除了Sigmoid函数，还有一个替代的方案是使用Rectified Linear Unit（ReLU）,ReLU的表达式是$c\ max(0,b+wx_1)$，从式子也很容易看出来它的形状，而这里如果我们把两个ReLU加起来，就可以得到一个Hard Sigmoid Function。这两种方式都是将模型复杂化以更贴近真实情况的方式，课程中使用的更多是ReLU的方式。

![](.\Resource\2.png)

实际上，我们可以更进一步，进行多次的ReLU嵌套（即将前一个ReLU函数的输出作为下一个函数的输入），这样可以认为是形成了多个layer，实验证明layer的增加同样会降低Loss的值，也就是说模型又进一步被优化了。这些一个个Sigmoid或者ReLU被称为Neuron（神经元），而一层层嵌套下去的这个网络，就被称为**Neural Network**，即神经网络。不过神经网络是很早就出现过的概念，而当时过于夸大导致名声并不好，后来又将每一层称为了hidden layer，而这一层一层的网络就更名为了**Deep Learning**。当然，并不是一味的增加层数就能产生更好的模型，当发现增加层数反而会增大Loss的时候，这个问题就被称为**Overfitting**（过拟合）。

如果到这里为止，可能对类似Sigmoid和ReLU这种函数的意义仍然不甚明朗，或者说不知道它具体在Deep Learning中起到了什么作用。如果我们把它放到神经元里去看，其实可以得到更好的理解：

![3](.\Resource\3.png)

实际上Sigmoid和ReLU这种函数在此处被称为Active Function（激活函数），如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

### 1.5 值得思考的问题（第一周）

- **问题1. 理解backpropagation数学原理，尝试查看pytorch中backward()源码实现**
- **问题2. 深入理解 logistic regression中的 discrimination和Generative两种方法**
- **问题3. 思考limitation of logistic regression 中的 feature transformation的线性代数相关知识，思考增加深度的作用，“升维”（增加特征（广度））是否可以解决这个limitation**



#### 1.5.1 关于Backpropagation的原理

关于backpropagation（反向传播算法），首先我们需要明确一点就是它是干什么用的。在第一节课中我们学到了Gradient decent的方法去更新参数，但是在一个复杂的神经网络中，参数的vector会非常的长，因此如何去计算这些参数对于Loss的偏微分就成了一个值得研究的问题。Backpropagation的目的就是为了去高效率的进行这个运算，它的原理是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。

首先理解一下链式法则：

![4](.\Resource\4.png)

所有参数对于Loss的偏微分在计算时，由于Loss是每个校验项的误差之和，表示为$L=\sum_{n=1}^{N}{C^n(\theta)}$，那么对于某一个参数来说，其偏微分就相当于对每一项误差的偏微分之和，也就是


$$
\frac{\delta L(\theta)}{\delta w}=\sum_{n=1}^{N}{\frac{\delta C^n(\theta)}{\delta w}}
$$


这里讨论的就是对某一个参数的偏微分是如何计算的，而重点就在于对某一组C（某一组误差）来说，参数的偏微分是多少。进一步的我们再微观一些讨论在某一个神经元处如何去计算我们想要的结果：

![5](.\Resource\5.png)

对于Forward pass步骤来说计算是非常简单的，因为很明显$\delta z / \delta w_1 = x_1$而$\delta z / \delta w_2 = x_2$，它的规律就是结果等于其与weight所链接的input，这里举例的是第一层的$w$，但是实际上对于后面层的计算也是一样的，因为每一层的输出依然是非常好计算的，而这个输出就是后面一层weight的Forward pass结果。但是问题是如何去计算Backward pass结果呢？

![6](.\Resource\6.png)

对于Backpropagation的步骤，依然使用链式法则的思路，假设激活函数的结果是a，那么$\frac{\delta a}{\delta z}$的结果是可计算的，仅仅是对激活函数求导，而对于后面的$\frac{\delta C} {\delta a}$来说，要求得a对该组验证数据的误差贡献如何，可以自然的向下面一层演算（这里举的例子是简单的两层），即延伸到$z'$和$z''$，而$\delta z' / \delta a$和$\delta z'' / \delta a$的结果是好计算的，分别是$w_3$和$w_4$，更难计算的部分在于后面的$\delta C/ \delta z'$和$\delta C/ \delta z''$，这里假设我们已经知道如何计算它了。这样我们就可以将需要的结果表示为：


$$
\frac{\delta C}{\delta z}=\sigma'(z)[w_3 \frac{\delta C}{\delta z'}+w_4\frac{\delta C}{\delta z''}]
$$


所以我们应该如何计算每一个激活函数中的$\frac{\delta C}{\delta z}$呢？我们可以将这个问题分为两种情况，第一种情况对于Output Layer来说，其实计算该值是非常容易的，通过链式法则我们可以将结果表示为（这里举例的是Output Layer中的某一个神经元处的激活函数）：


$$
\frac{\delta C}{\delta z'}= \frac{\delta y_1}{\delta z'}\frac{\delta C}{\delta y_1} \qquad \frac{\delta C}{\delta z''}= \frac{\delta y_2}{\delta z''}\frac{\delta C}{\delta y_2}
$$


对于前面一项来说，它只取决于激活函数，而对于后一项来说它只取决于定义的计算误差的函数，这两者都是已经确定的，因此计算它们的微分并不困难。问题在于如果不是计算最后一层呢？对于任何一个中间层来说，它的结构是如下所示的：

![7](.\Resource\7.png)

如果我们回头去看上面的公式，会发现实际上某一处需要计算的$\frac{\delta C}{\delta z}$是可以通过后面一层的$\delta C/ \delta z'$和$\delta C/ \delta z''$得来的，而如果这样一层一层的推下去，一直推到最后一层输出层，它的结果在刚才讨论过，是可计算的，因此这整个过程相当于一个递归，一直到最后的输出层为止，然后一层层回溯。而如果我们从后往前去计算，这个计算过程就变得简单了许多，我们就像是建立了一个反向的神经网络，从输出层开始去反向的计算。总结来说，整个过程就如下图所示：

![8](.\Resource\8.png)

#### 1.5.2 Pytorch中对于Backpropagation的实现

那么在Pytorch中是怎么实现这个过程的呢？首先我们需要明确的是，Pytorch的计算图的搭建和运算是同时的，随时可以输出结果，在计算图中只有两种元素，分别是数据（tensor）和运算（operation），而数据可以分为叶子节点和非叶子节点。其中叶子节点指的是用户创建的节点，不依赖于其他节点。而在反向传播结束后，非叶子节点的梯度会被释放掉，只保留叶子节点的梯度从而节省内存，而如果想保存非叶子节点的梯度可以使用`retain_grad()`的方法。而对于tensor来说，它有几个常用的属性：

- 查看是否可以求导 `requires_grad` （对于叶子节点默认False，非叶子节点为True）
- 查看运算名称 `grad_fn`
- 查看是否为叶子节点 `is_leaf`
- 查看导数值 `grad`

当我们需要对某个Tensor变量求梯度的时候需要先指定`requires_grad`属性为True，指定方式可以是如下两种：

```python
x = torch.tensor(1.).requires_grad_()

x = torch.tensor(1., requires_grad=True)
```

Pytorch提供了两种求梯度的方法，分别是`backward()`和`torch.autograd.grad()`，他们的区别在于前者是给叶子节点填充grad字段，而后者是直接返回梯度，实际上`y.backward()`其实等同于`torch.autograd.backward(y)`，使用backward()函数反向传播计算tensor的梯度时，并不计算所有tensor的梯度，而是只计算满足这几个条件的tensor的梯度：1.类型为叶子节点、2.requires_grad=True、3.依赖该tensor的所有tensor的requires_grad=True。所有满足条件的变量梯度会自动保存到对应的`grad`属性里。举个简单的例子，给定$y=(x+1)*(x+2)$并计算$\delta y / \delta x$，假设给定了$x=2$，使用backward函数的方式是：

```python
x = torch.tensor(2., requires_grad=True)

a = torch.add(x, 1)
b = torch.add(x, 2)
y = torch.mul(a, b)

y.backward()
print(x.grad)
>>>tensor(7.)

print("requires_grad: ", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)
print("is_leaf: ", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)
print("grad: ", x.grad, a.grad, b.grad, y.grad)’

>>>requires_grad:  True True True True
>>>is_leaf:  True False False False
>>>grad:  tensor(7.) None None None
```

而如果我们使用`autograd.grad()`的方式，会是如下所示：

```python
x = torch.tensor(2., requires_grad=True)

a = torch.add(x, 1)
b = torch.add(x, 2)
y = torch.mul(a, b)

grad = torch.autograd.grad(outputs=y, inputs=x)
print(grad[0])
>>>tensor(7.)
```

这里指定了输出是y而输入是x，所以函数的返回值就会是$\delta y / \delta x$这一梯度，完整的返回值其实是一个元组但是我们只用保留第一个元素，后面的元素全部都是`?`。进一步我们研究更复杂并包含高阶求导的一个例子：$z = x^2y$，计算$\delta z / \delta x, \delta z / \delta y,\delta^2 z / \delta^2 x$，并假设$x=2,y=3$，求一阶导时两种方式都可以使用：

```python
x = torch.tensor(2., requires_grad=True)
y = torch.tensor(3., requires_grad=True)

z = x * x * y

z.backward() #使用backward的方式
print(x.grad, y.grad)
>>>tensor(12.) tensor(4.)

grad_x = torch.autograd.grad(outputs=z, inputs=x) #使用autograd.grad的方式
print(grad_x[0])
>>>tensor(12.)
```

我们会发现在使用`autograd.grad()`的时候没有同时计算y的导数，这是因为无论是`backward`还是`autograd.grad`在计算一次梯度后图就被释放了，如果想要保留，需要添加`retain_graph=True`，如下所示：

```python
x = torch.tensor(2.).requires_grad_()
y = torch.tensor(3.).requires_grad_()

z = x * x * y

grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)
grad_y = torch.autograd.grad(outputs=z, inputs=y)

print(grad_x[0], grad_y[0])
>>>tensor(12.) tensor(4.) 
```

继续探究如何去求高阶导，理论上来说是上面的`grad_x`再对$x$求梯度，但是如果我们直接求的话会发现报错，因为虽然`retain_graph=True`保留了计算图和中间变量梯度， 但没有保存`grad_x`的运算方式，需要使用`creat_graph=True`在保留原图的基础上再建立额外的求导计算图，也就是会把$\delta z/\delta x=2xy$ 这样的运算存下来，反应到代码里是这样操作的：

```python
x = torch.tensor(2.).requires_grad_()
y = torch.tensor(3.).requires_grad_()

z = x * x * y

# 使用两次autograd.grad()
grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)
grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)
print(grad_xx[0])

# autograd.grad() + backward()
grad = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)
grad[0].backward()
print(x.grad)

# backward() + autograd.grad()
z.backward(create_graph=True)
grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)
print(grad_xx[0])

# 使用两次backward()
z.backward(create_graph=True)
x.grad.data.zero_() #梯度清零
x.grad.backward()
print(x.grad)
```

上面的代码块给出了所有四种可能的计算方式，它们的结果是相同的，都能得出正确的结果6，只是使用时的语法不同，但是尤其需要注意在第四种方式的时候，连续使用backward中途一定要把**梯度清零**，否则会默认累加梯度，这样在这个例子中算出来的结果会是18（因为第一阶导数的结果是12，而第二阶导数的结果是6，累加之后就是18了），在神经网络中，我们只需要执行`optimizer.zero_grad()`方法就可以了。

**前面所有的例子里面，都是对标量（scalar）求导，那如果要对向量（Tensor）求导呢？**

如果直接尝试使用同样的语法，会报错。因为只能标量对标量，标量对向量求梯度，所以如果要求导，需要先将$y$转换为标量，对分别求导没影响的就是求和。还是给出一个例子辅助理解：

```python3
x = torch.tensor([1., 2.]).requires_grad_()
y = x * x

y.sum().backward()
print(x.grad)
>>>tensor([2., 4.])
```

这里使用了sum函数，这是因为此时$x=[x_1,x_2], \ y =[x^2_1,x^2_2],\ y'=y.sum()=x^2_1+x^2_2$，如果再具体一些来解释的话，结合雅可比矩阵来理解，已知$y=[y_1,y_2]$是个向量，则：


$$
J=[\frac{\delta y}{\delta x_1},\frac{\delta y}{\delta x_2}]=\left[\begin{matrix} \frac{\delta y_1}{\delta x_1} & \frac{\delta y_1}{\delta x_2} \\ \frac{\delta y_2}{\delta x_1} & \frac{\delta y_2}{\delta x_2}\end{matrix}\right]
$$


而我们希望最终求导的结果是$\frac{\delta y_1}{\delta x_1},\frac{\delta y_2}{\delta x_2}$，注意到$\frac{\delta y_1}{\delta x_2}$和$\frac{\delta y_2}{\delta x_1}$都是0，那么我们就可以用如下计算来解决：




$$
[\frac{\delta y_1}{\delta x_1},\frac{\delta y_2}{\delta x_2}]=[1,1]\left[\begin{matrix} \frac{\delta y_1}{\delta x_1} & \frac{\delta y_1}{\delta x_2} \\ \frac{\delta y_2}{\delta x_1} & \frac{\delta y_2}{\delta x_2}\end{matrix}\right] \ (vector-Jacobian\ product)
$$


因此如果我们不使用sum函数，另一种方式是：

```python
x = torch.tensor([1., 2.]).requires_grad_()
y = x * x

# 使用backward
y.backward(torch.ones_like(y)) #此处的参数就是雅可比矩阵左乘的那个向量
print(x.grad)
>>>tensor([2., 4.])

# 使用autograd.grad()
grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))
print(grad_x[0])
>>>tensor([2., 4.])
```

至此，我们好像已经明白了backward的用法，但是如果回头去看1.5.1中解释的backpropagation的过程，会发现如果要得到我们最关心的gradient decent，除了backward的过程不是还有forward的过程么？如果结合具体的神经网络模型来看，backward函数又起到了怎样的作用呢？为什么仅仅是这样一行代码我们便得到了自己想要的结果呢？

实际上，对于真实的神经网络来说，它只是在Function set部分更加复杂一些，而pytorch所做的事情仍就是在模型定义后，正向计算output的同时去记录每一步的操作，这里就形成了一个重要的概念，即**计算图**。而在调用backward方法的时候，则根据backpropagation的思路去计算偏微分，会从根节点（输出节点）开始反向构建计算图，即从根节点开始遍历这些节点来构造一个反向传播梯度的计算图模型，将计算得到的梯度值更新到上一层的节点，并重复此过程直至所有required=True的tensor变量都得到更新。从输出节点(根)遍历tensors，使用了栈结构，每个tensor梯度计算的具体方法存放于tensor节点的grad_fn属性中（这就是之前提到过的，实际上定义好了每一步的计算方式后，这一步的微分就是确定的，作为路径的一部分可以提前储存），依据此构建出包含梯度计算方法的反向传播计算图。

参考资料：

【1】[一文解释PyTorch求导相关 (backward, autograd.grad) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/279758736)

【2】[(9条消息) 06 Pytorch实现反向传播_蓝子娃娃的博客-CSDN博客_pytorch 反向传播](https://blog.csdn.net/qq_41033011/article/details/109325070)