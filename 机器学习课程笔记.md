# 机器学习课程笔记

课程链接：[ML 2022 Spring (ntu.edu.tw)](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)

## 一、机器学习、深度学习的基本概念

什么是机器学习？用一句话来描述，就是让机器拥有去寻找函数的能力。（例如，寻找可以将声音信号识别成文字的函数）我们给出一个输入，通过机器学习来构造一个函数来获取我们需要的输出，下面是不同的函数类型：

**Regression**: The function outputs a scalar （输出一个数值的函数）

**Classification**: Given options (classes), the function outputs the correct one （分类、区分的函数，例如Alpha Go下围棋的过程其实也是Classification，给出棋盘上19*19的选项去选出最佳的一项）

### 1.1 关于预测频道观看人数的例子

能否使用机器学习的方法，根据历史的观看人数，去预测某频道接下来一天的观看人数？如果要做这样一件事，我们可以先去思考这样的一个函数会是什么样子的：

假设$y$是我们想知道的结果（预测的观看人数），$x_1$ 是我们已知的信息（历史数据），可以推测可能的函数构造是$y=b+wx_1$ 其中b和$w$就是我们未知的参数。而如何去获取这些未知的参数就需要一些Domain knowledge，即对于具体问题的一些背景知识理解。

这个带有未知参数的Function，在机器学习中就被称为**Model**（模型），而$x_1$则被称为**Feature**，$w$被称为**Weight**，b则被称为**Bias**。另外一个重要的概念是**Loss**，它是一个函数，接收之前提到的未知参数为输入，并且评估输入的一组参数有多好，可以表示为$L=(b,w)$。这样描述还是略微抽象，可以结合本节的例子更好的理解Loss

### 1.2 如何理解Loss

假设我们知道了一组未知参数（作为Loss的输入），我们就可以构造出一个暂时的Function，而我们拥有历史的真实数据，对每一天的观看人数，我们可以使用构造出的Function去进行运算并得出后一天的预测人数，而对于历史数据来说后一天的真实人数是已知的，因此对于第n天的预测结果我们会得到一个差值$e_n$，当然这个差值不一定是简单的相减，只需要理解为真实值和预测值之间的误差程度，而对于很多天的历史数据而言，我们会计算出很多个差值，Loss也可以表示为$L=\frac{1}{N}\sum_{i=1}^{n}{e_i}$，很好理解的是当L越大说明函数的效果越差。

如果用差值的绝对值去计算误差，得出的Loss会被称为Mean Absolute Error（MAE），如果用差值的平方的方式去计算，则会被称为Mean Square Error（MSE）

MAE和MSE的区别：[机器学习常用损失函数小结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/77686118)



### 1.3  关于Optimization

这一步是机器学习的第三步，但我们已知了如何去评判选出参数的表现的时候，我们的任务就变成了如何选取最佳的参数。这个过程就是Optimization。在这一步里面我们用到的方法是**Gradient Descent**（梯度下降法），它的步骤可以简述为（这里简化了，只考虑了$w$而忽略了b）：

- 随机选取一个初始值$w^0$
- 计算w对Loss的微分是多少：$\frac{\delta L}{\delta w}|_{w=w^0}$ 或者说就是计算在$w^0$处的切线斜率
- 很明显，当斜率为正的时候，减小$w$的值会降低Loss，当斜率为负的时候，增加$w$的值会降低Loss 
- 如何确定$w$的值要更改多少呢？它的表达是$\eta\frac{\delta L}{\delta w}|_{w=w^0}$，这里$\eta$是learning rate，该值需要自己去确定
- 根据$w^1 = w^0 -\eta\frac{\delta L}{\delta w}|_{w=w^0}$更新$w$的值
- 如果当斜率为零的时候，此时$w$的值也不能再改变，这个点称为**Local minima**，但是注意这个点可能并不是Loss最小的点，使得Loss最小的点被称为**Global minima**

这个过程也可以很容易的推广到考虑多个参数的情况，并且计算斜率的过程在大多数的深度学习框架里都包装成了简单的方法。

继续回到之前提到的例子，我们会发现整个训练过程完成后，得出的参数所构造的函数其实表现并不尽人意。这也很好理解，因为我们所假设的函数格式过于简单了，我们只是简单的认为后一天的观看人数和前一天的相关。如果我们考虑更多的数据，去将历史数据更充分的使用，例如使用前七天的观看人数去预测，将函数假设为如下结构：


$$
y= b + \sum_{j=1}^{7}{w_j}{i_j}
$$


我们会发现Loss的值会更小，也就是说训练出来模型的效果更好了。我们甚至可以再进一步，去用更多的历史数据，例如28天、56天甚至更多，也会有更好的模型效果（但是可能到某一个点再继续升高日子并不会提升了）。基于这个思路去假设的函数结构被称为**Linear Model**（线性模型），而这种结果也确实太过简单了。这种由于模型结构过于简单而产生的问题被称为Model Bias，即模型无法去模拟真实的状况。

### 1.4 进一步复杂的Function

前面提到我们使用线性的模型时，无论怎样调整参数，得出的图形也只会是一条直线。而很显然现实情况往往没有这么简单，关于观看人数的曲线呈现一个更复杂的形状（例如多个转折的曲线），我们可以尝试将预测模型更换为一个常数和一组折线的和，而这个折线往往表示为一条S形的曲线，称为Sigmoid Function，它的公式表达是：


$$
y= c \frac{1}{1+e^{-(b+wx_1)}}= c\ sigmoid(b+wx_1)
$$


这样的改变可以理解为，我们带入了更多的Feature，从而减小Model Bias，相应的模型可以表示为：


$$
y=b+\sum_{i}{c_i \ sigmoid(b_i+\sum_{j}{w_{ij}x_j})}
$$


这里的$i$是sigmoid函数的编号，而$j$则是feature的编号（这里的x就是日期），下面将这个模型表示为了图示和更简洁的公式表示，其中右侧的b，W和x是矩阵表示，因为括号内部的累加可以表示为矩阵乘法。而经过sigmoid方程后它们则表现为三条不同的曲线。

![](.\Resource\1.png)

这样的表示也带来了更多的参数，除了x为feature以外，其他都是需要确定的参数，注意这里的W等都是矩阵，因此将它们展开实际上是多个需要确定的参数，我们把这些参数全部列出来，归纳表达为$\theta$，同样我们需要去计算Loss来评价这些参数的表现。

同样的，在选取一个初始值之后，对于每个参数，都需要去与L进行微分计算，这样就得出了一个向量，内部是所有参数对L做微分计算的结果，表示为：$g=\nabla L(\theta^0)$，这里用的是初始点做例子。而在更新过程中，会根据learning rate同时更新所有的参数，即$\theta^0-\eta g \rightarrow \theta^1$。而在实际操作时，往往数据量会非常大，而常见的作法是将其分为多个**batch**，而每次计算Loss的时候使用不同的batch，当使用完一轮batch的时候就称为一个**epoch**。例如有一万条数据，而一个Batch的数量是10，则有1000个batch，那么每一个epoch内就更新了1000次参数。

除了Sigmoid函数，还有一个替代的方案是使用Rectified Linear Unit（ReLU）,ReLU的表达式是$c\ max(0,b+wx_1)$，从式子也很容易看出来它的形状，而这里如果我们把两个ReLU加起来，就可以得到一个Hard Sigmoid Function。这两种方式都是将模型复杂化以更贴近真实情况的方式，课程中使用的更多是ReLU的方式。

![](.\Resource\2.png)

实际上，我们可以更进一步，进行多次的ReLU嵌套（即将前一个ReLU函数的输出作为下一个函数的输入），这样可以认为是形成了多个layer，实验证明layer的增加同样会降低Loss的值，也就是说模型又进一步被优化了。这些一个个Sigmoid或者ReLU被称为Neuron（神经元），而一层层嵌套下去的这个网络，就被称为**Neural Network**，即神经网络。不过神经网络是很早就出现过的概念，而当时过于夸大导致名声并不好，后来又将每一层称为了hidden layer，而这一层一层的网络就更名为了**Deep Learning**。当然，并不是一味的增加层数就能产生更好的模型，当发现增加层数反而会增大Loss的时候，这个问题就被称为**Overfitting**（过拟合）。

如果到这里为止，可能对类似Sigmoid和ReLU这种函数的意义仍然不甚明朗，或者说不知道它具体在Deep Learning中起到了什么作用。如果我们把它放到神经元里去看，其实可以得到更好的理解：

![3](.\Resource\3.png)

实际上Sigmoid和ReLU这种函数在此处被称为Active Function（激活函数），如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

### 1.5 值得思考的问题（第一周）

- **问题1. 理解backpropagation数学原理，尝试查看pytorch中backward()源码实现**
- **问题2. 深入理解 logistic regression中的 discrimination和Generative两种方法**
- **问题3. 思考limitation of logistic regression 中的 feature transformation的线性代数相关知识，思考增加深度的作用，“升维”（增加特征（广度））是否可以解决这个limitation**



#### 1.5.1 关于Backpropagation的原理

关于backpropagation（反向传播算法），首先我们需要明确一点就是它是干什么用的。在第一节课中我们学到了Gradient decent的方法去更新参数，但是在一个复杂的神经网络中，参数的vector会非常的长，因此如何去计算这些参数对于Loss的偏微分就成了一个值得研究的问题。Backpropagation的目的就是为了去高效率的进行这个运算，它的原理是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。

首先理解一下链式法则：

![4](.\Resource\4.png)

所有参数对于Loss的偏微分在计算时，由于Loss是每个校验项的误差之和，表示为$L=\sum_{n=1}^{N}{C^n(\theta)}$，那么对于某一个参数来说，其偏微分就相当于对每一项误差的偏微分之和，也就是


$$
\frac{\delta L(\theta)}{\delta w}=\sum_{n=1}^{N}{\frac{\delta C^n(\theta)}{\delta w}}
$$


这里讨论的就是对某一个参数的偏微分是如何计算的，而重点就在于对某一组C（某一组误差）来说，参数的偏微分是多少。进一步的我们再微观一些讨论在某一个神经元处如何去计算我们想要的结果：

![5](.\Resource\5.png)

对于Forward pass步骤来说计算是非常简单的，因为很明显$\delta z / \delta w_1 = x_1$而$\delta z / \delta w_2 = x_2$，它的规律就是结果等于其与weight所链接的input，这里举例的是第一层的$w$，但是实际上对于后面层的计算也是一样的，因为每一层的输出依然是非常好计算的，而这个输出就是后面一层weight的Forward pass结果。但是问题是如何去计算Backward pass结果呢？

![6](.\Resource\6.png)

对于Backpropagation的步骤，依然使用链式法则的思路，假设激活函数的结果是a，那么$\frac{\delta a}{\delta z}$的结果是可计算的，仅仅是对激活函数求导，而对于后面的$\frac{\delta C} {\delta a}$来说，要求得a对该组验证数据的误差贡献如何，可以自然的向下面一层演算（这里举的例子是简单的两层），即延伸到$z'$和$z''$，而$\delta z' / \delta a$和$\delta z'' / \delta a$的结果是好计算的，分别是$w_3$和$w_4$，更难计算的部分在于后面的$\delta C/ \delta z'$和$\delta C/ \delta z''$，这里假设我们已经知道如何计算它了。这样我们就可以将需要的结果表示为：


$$
\frac{\delta C}{\delta z}=\sigma'(z)[w_3 \frac{\delta C}{\delta z'}+w_4\frac{\delta C}{\delta z''}]
$$


所以我们应该如何计算每一个激活函数中的$\frac{\delta C}{\delta z}$呢？我们可以将这个问题分为两种情况，第一种情况对于Output Layer来说，其实计算该值是非常容易的，通过链式法则我们可以将结果表示为（这里举例的是Output Layer中的某一个神经元处的激活函数）：


$$
\frac{\delta C}{\delta z'}= \frac{\delta y_1}{\delta z'}\frac{\delta C}{\delta y_1} \qquad \frac{\delta C}{\delta z''}= \frac{\delta y_2}{\delta z''}\frac{\delta C}{\delta y_2}
$$


对于前面一项来说，它只取决于激活函数，而对于后一项来说它只取决于定义的计算误差的函数，这两者都是已经确定的，因此计算它们的微分并不困难。问题在于如果不是计算最后一层呢？对于任何一个中间层来说，它的结构是如下所示的：

![7](.\Resource\7.png)

如果我们回头去看上面的公式，会发现实际上某一处需要计算的$\frac{\delta C}{\delta z}$是可以通过后面一层的$\delta C/ \delta z'$和$\delta C/ \delta z''$得来的，而如果这样一层一层的推下去，一直推到最后一层输出层，它的结果在刚才讨论过，是可计算的，因此这整个过程相当于一个递归，一直到最后的输出层为止，然后一层层回溯。而如果我们从后往前去计算，这个计算过程就变得简单了许多，我们就像是建立了一个反向的神经网络，从输出层开始去反向的计算。总结来说，整个过程就如下图所示：

![8](.\Resource\8.png)

#### 1.5.2 Pytorch中对于Backpropagation的实现

那么在Pytorch中是怎么实现这个过程的呢？首先我们需要明确的是，Pytorch的计算图的搭建和运算是同时的，随时可以输出结果，在计算图中只有两种元素，分别是数据（tensor）和运算（operation），而数据可以分为叶子节点和非叶子节点。其中叶子节点指的是用户创建的节点，不依赖于其他节点。而在反向传播结束后，非叶子节点的梯度会被释放掉，只保留叶子节点的梯度从而节省内存，而如果想保存非叶子节点的梯度可以使用`retain_grad()`的方法。而对于tensor来说，它有几个常用的属性：

- 查看是否可以求导 `requires_grad` （对于叶子节点默认False，非叶子节点为True）
- 查看运算名称 `grad_fn`
- 查看是否为叶子节点 `is_leaf`
- 查看导数值 `grad`

当我们需要对某个Tensor变量求梯度的时候需要先指定`requires_grad`属性为True，指定方式可以是如下两种：

```python
x = torch.tensor(1.).requires_grad_()

x = torch.tensor(1., requires_grad=True)
```

Pytorch提供了两种求梯度的方法，分别是`backward()`和`torch.autograd.grad()`，他们的区别在于前者是给叶子节点填充grad字段，而后者是直接返回梯度，实际上`y.backward()`其实等同于`torch.autograd.backward(y)`，使用backward()函数反向传播计算tensor的梯度时，并不计算所有tensor的梯度，而是只计算满足这几个条件的tensor的梯度：1.类型为叶子节点、2.requires_grad=True、3.依赖该tensor的所有tensor的requires_grad=True。所有满足条件的变量梯度会自动保存到对应的`grad`属性里。举个简单的例子，给定$y=(x+1)*(x+2)$并计算$\delta y / \delta x$，假设给定了$x=2$，使用backward函数的方式是：

```python
x = torch.tensor(2., requires_grad=True)

a = torch.add(x, 1)
b = torch.add(x, 2)
y = torch.mul(a, b)

y.backward()
print(x.grad)
>>>tensor(7.)

print("requires_grad: ", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)
print("is_leaf: ", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)
print("grad: ", x.grad, a.grad, b.grad, y.grad)’

>>>requires_grad:  True True True True
>>>is_leaf:  True False False False
>>>grad:  tensor(7.) None None None
```

而如果我们使用`autograd.grad()`的方式，会是如下所示：

```python
x = torch.tensor(2., requires_grad=True)

a = torch.add(x, 1)
b = torch.add(x, 2)
y = torch.mul(a, b)

grad = torch.autograd.grad(outputs=y, inputs=x)
print(grad[0])
>>>tensor(7.)
```

这里指定了输出是y而输入是x，所以函数的返回值就会是$\delta y / \delta x$这一梯度，完整的返回值其实是一个元组但是我们只用保留第一个元素，后面的元素全部都是`?`。进一步我们研究更复杂并包含高阶求导的一个例子：$z = x^2y$，计算$\delta z / \delta x, \delta z / \delta y,\delta^2 z / \delta^2 x$，并假设$x=2,y=3$，求一阶导时两种方式都可以使用：

```python
x = torch.tensor(2., requires_grad=True)
y = torch.tensor(3., requires_grad=True)

z = x * x * y

z.backward() #使用backward的方式
print(x.grad, y.grad)
>>>tensor(12.) tensor(4.)

grad_x = torch.autograd.grad(outputs=z, inputs=x) #使用autograd.grad的方式
print(grad_x[0])
>>>tensor(12.)
```

我们会发现在使用`autograd.grad()`的时候没有同时计算y的导数，这是因为无论是`backward`还是`autograd.grad`在计算一次梯度后图就被释放了，如果想要保留，需要添加`retain_graph=True`，如下所示：

```python
x = torch.tensor(2.).requires_grad_()
y = torch.tensor(3.).requires_grad_()

z = x * x * y

grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)
grad_y = torch.autograd.grad(outputs=z, inputs=y)

print(grad_x[0], grad_y[0])
>>>tensor(12.) tensor(4.) 
```

继续探究如何去求高阶导，理论上来说是上面的`grad_x`再对$x$求梯度，但是如果我们直接求的话会发现报错，因为虽然`retain_graph=True`保留了计算图和中间变量梯度， 但没有保存`grad_x`的运算方式，需要使用`creat_graph=True`在保留原图的基础上再建立额外的求导计算图，也就是会把$\delta z/\delta x=2xy$ 这样的运算存下来，反应到代码里是这样操作的：

```python
x = torch.tensor(2.).requires_grad_()
y = torch.tensor(3.).requires_grad_()

z = x * x * y

# 使用两次autograd.grad()
grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)
grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)
print(grad_xx[0])

# autograd.grad() + backward()
grad = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)
grad[0].backward()
print(x.grad)

# backward() + autograd.grad()
z.backward(create_graph=True)
grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)
print(grad_xx[0])

# 使用两次backward()
z.backward(create_graph=True)
x.grad.data.zero_() #梯度清零
x.grad.backward()
print(x.grad)
```

上面的代码块给出了所有四种可能的计算方式，它们的结果是相同的，都能得出正确的结果6，只是使用时的语法不同，但是尤其需要注意在第四种方式的时候，连续使用backward中途一定要把**梯度清零**，否则会默认累加梯度，这样在这个例子中算出来的结果会是18（因为第一阶导数的结果是12，而第二阶导数的结果是6，累加之后就是18了），在神经网络中，我们只需要执行`optimizer.zero_grad()`方法就可以了。

**前面所有的例子里面，都是对标量（scalar）求导，那如果要对向量（Tensor）求导呢？**

如果直接尝试使用同样的语法，会报错。因为只能标量对标量，标量对向量求梯度，所以如果要求导，需要先将$y$转换为标量，对分别求导没影响的就是求和。还是给出一个例子辅助理解：

```python3
x = torch.tensor([1., 2.]).requires_grad_()
y = x * x

y.sum().backward()
print(x.grad)
>>>tensor([2., 4.])
```

这里使用了sum函数，这是因为此时$x=[x_1,x_2], \ y =[x^2_1,x^2_2],\ y'=y.sum()=x^2_1+x^2_2$，如果再具体一些来解释的话，结合雅可比矩阵来理解，已知$y=[y_1,y_2]$是个向量，则：


$$
J=[\frac{\delta y}{\delta x_1},\frac{\delta y}{\delta x_2}]=\left[\begin{matrix} \frac{\delta y_1}{\delta x_1} & \frac{\delta y_1}{\delta x_2} \\ \frac{\delta y_2}{\delta x_1} & \frac{\delta y_2}{\delta x_2}\end{matrix}\right]
$$


而我们希望最终求导的结果是$\frac{\delta y_1}{\delta x_1},\frac{\delta y_2}{\delta x_2}$，注意到$\frac{\delta y_1}{\delta x_2}$和$\frac{\delta y_2}{\delta x_1}$都是0，那么我们就可以用如下计算来解决：




$$
[\frac{\delta y_1}{\delta x_1},\frac{\delta y_2}{\delta x_2}]=[1,1]\left[\begin{matrix} \frac{\delta y_1}{\delta x_1} & \frac{\delta y_1}{\delta x_2} \\ \frac{\delta y_2}{\delta x_1} & \frac{\delta y_2}{\delta x_2}\end{matrix}\right] \ (vector-Jacobian\ product)
$$


因此如果我们不使用sum函数，另一种方式是：

```python
x = torch.tensor([1., 2.]).requires_grad_()
y = x * x

# 使用backward
y.backward(torch.ones_like(y)) #此处的参数就是雅可比矩阵左乘的那个向量
print(x.grad)
>>>tensor([2., 4.])

# 使用autograd.grad()
grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))
print(grad_x[0])
>>>tensor([2., 4.])
```

至此，我们好像已经明白了backward的用法，但是如果回头去看1.5.1中解释的backpropagation的过程，会发现如果要得到我们最关心的gradient decent，除了backward的过程不是还有forward的过程么？如果结合具体的神经网络模型来看，backward函数又起到了怎样的作用呢？为什么仅仅是这样一行代码我们便得到了自己想要的结果呢？

实际上，对于真实的神经网络来说，它只是在Function set部分更加复杂一些，而pytorch所做的事情仍就是在模型定义后，正向计算output的同时去记录每一步的操作，这里就形成了一个重要的概念，即**计算图**。而在调用backward方法的时候，则根据backpropagation的思路去计算偏微分，会从根节点（输出节点）开始反向构建计算图，即从根节点开始遍历这些节点来构造一个反向传播梯度的计算图模型，将计算得到的梯度值更新到上一层的节点，并重复此过程直至所有required=True的tensor变量都得到更新。从输出节点(根)遍历tensors，使用了栈结构，每个tensor梯度计算的具体方法存放于tensor节点的grad_fn属性中（这就是之前提到过的，实际上定义好了每一步的计算方式后，这一步的微分就是确定的，作为路径的一部分可以提前储存），依据此构建出包含梯度计算方法的反向传播计算图。

// TODO

继续深入探究pytorch中backward的过程和**源码**

参考资料：

【1】[一文解释PyTorch求导相关 (backward, autograd.grad) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/279758736)

【2】[(9条消息) 06 Pytorch实现反向传播_蓝子娃娃的博客-CSDN博客_pytorch 反向传播](https://blog.csdn.net/qq_41033011/article/details/109325070)



## 二、Logistic Regression相关知识

本节是第一周课程后两个问题的引申，对应着课程拓展素材中的几个视频。

在第一周的课程里举的例子是预测频道观看人数，它是一个典型的Linear Regression（线性回归）的问题，所谓线性回归问题，它的主要功能是用来**拟合数据**，而其方法在一中已经介绍的非常完备了，而这里引出的第二种回归问题，是逻辑回归，它的目的是为了**区分数据**，找到决策边界，它处理的是预测值为一个离散变量的情况。在课程中老师举得例子是给定一个神奇宝贝的各种数值，去判断它的种族（对线性回归对应的例子是，给定神奇宝贝的CP值和其他能力值，去预测进化后的CP值）

课程链接（第一周课程的拓展）:

- Pokemon Classification: https://youtu.be/fZAZUYEeIMg

- Logistic Regression: https://youtu.be/hSXFuypLukA

### 2.1 关于判断神奇宝贝种族的例子

给定这样一个问题场景：已知神奇宝贝的一些数值（例如CP、生命值、攻击力、防御力等）来判断一个未知种族的神奇宝贝属于哪个种族，这个问题应该如何利用机器学习去解决呢？首先，和线性回归不同的一点是，我们需要的输出是一个离散的结果（种类）而不是一个连续的数值结果。因此如果只是单纯的将每个类别定义为一个target数值的话，实际上是不合适的，因为这样相当于认为某两个类会更相关（数值更接近），然而一旦这种假设是错误的，那么问题就无法得到很好的解决。所以我们理想中的方法应该是这样的：

![9](.\Resource\9.png)

但是本周的课程先不讲到SVM相关的解决方案，而是尝试先用概率的思路去解决该问题。可以想象一个场景，盒子1中有四个蓝球和一个绿球，盒子2中有2个蓝球3个绿球，那么从两个盒子中取一个蓝球，它是从盒子1中取出来的概率是多少呢？这个问题很好解决：


$$
P(B_1 | Blue)=\frac{P(Blue|B_1)P(B_1)}{P(Blue|B_1)P(B_1)+P(Blue|B_2)P(B_2)}
$$


我们可以把这个问题发散到我们需要解决的问题上，也就是说给出一个x，去判断它是从哪个类中抽取出来的，因此我们需要知道从每个类中抽出x的几率，然后利用上面提到的公式，即抽出一个指定的x，它从某个类中抽出的概率，可以得到我们想要的结果。这一套的想法得到的Model被称为**Generative Model**，之所以叫这个名字，是因为根据前面的概率公式，我们只需要知道每个类被选择的几率和每个类中选出x的几率就可以sample出某一个x出现的几率，就可以自己产生x（之后会有更详细的对比）。回到具体问题上来，假设我们现在需要区分水系和普通系的神奇宝贝，用防御力和特殊防御力这两个数值作为x，画出它们的分布图，我们尝试使用**Gaussian Distribution**去sample 它们，这里不详细的去解释高斯分布的概念，主要注意：

- 输入为一个vector x，输出为sample出x的概率
- $\mu$决定了中心点，$\Sigma$代表covariance matrix，决定了分散程度
- 由于这里考虑的是二维的情况，$\Sigma$是一个2维矩阵
- 它的具体公式是：$f_{\mu,\Sigma}=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]$

下面问题就转化为了怎么去找一个合适的$\mu$和$\Sigma$，这里使用的方法叫做Maximum Likelihood，实际上无论我们取什么值，都可以sample出任意的x，只不过几率会有差别，假设我们现在有79个数据点，选定一个高斯分布的参数，那么每个点被sample出来的几率我们是知道的。我们的目的就是让选定的高斯分布sample出所有x的可能性最大，而计算方法也非常直观，只需要将所有的x被sample出来的几率相乘。这里的结果非常直接，对所有的x取平均值就得到了最佳的$\mu$，而求出之后也可以计算最佳的$\Sigma$（这里以79个数据为例）：


$$
\mu^*=\frac{1}{79}\sum^{79}_{n=1}x^n \qquad \Sigma^*=\frac{1}{79}\sum^{79}_{n=1}(x^n-\mu^*)(x^n-\mu^*)^T
$$


这里是简化了过程，实际上按照求导去计算的结果也是相同的。那么我们拿到最佳的高斯分布之后就可以去进行之前提到的概率公式计算了：

![10](.\Resource\10.png)

但是做到这一步，如果去看结果的话，在例子上的表现并不是很好，正确率只有47%，不过这里我们只是使用了二维空间的信息（防御力和特殊防御力），我们还可以利用更多维的输入作为判断依据。还有一个优化思路是使用更少的参数以避免过拟合，比如说我们可以故意的对两个类使用相同的$\Sigma$，当然我们还是要像正常情况下去进行maximum likelihood的方式去推演最好的参数。这里$\mu$的计算方式是一样的，但是$\Sigma$的计算方式会是原本两个类别的$\Sigma$的加权平均。 这样操作之后，划分的界限会变成一条直线（在二维中）：

![11](.\Resource\11.png)

实际上，选取什么样的概率分布是我们自己去选择的，这里选择的高斯分布只是一个例子，比方说对于binary的特征来说我们会认为它是服从伯努利分布的。对刚才的例子再深入讨论下，进行如下的操作：



$$
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|c_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}=\frac{1}{1+exp(-z)}=\sigma(z)
$$



这里的$\sigma$指的就是sigmoid function，而经历一系列的计算之后，会得出z的格式实际上还是可以表现为$z=w^Tx+b$的形式（基于$\Sigma_1=\Sigma_2$），这也解释了为什么分界线会是一条直线的形式，那么其中的$w$和$b$能不能直接找出来呢？

### 2.2 进一步的Logistic Regression

为了直接找到$w$和$b$，我们假设$f_{w,b}(x)=P_{w,b}(x|C_1)$来定义出我们所需要的Function set：

![12](.\Resource\12.png)

这里的模型就是Logistic Regression，而下一步仍然是需要评估Function的好坏，对于训练数据来说，它具备x和正确的种类，给定一组参数，就可以计算出产生正确结果的几率，例如这里讨论只有两种类型的情况，计算出了Likelihood可以表示为：


$$
L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))...
$$


这里假设的是x1和x2属于Class 1而x3属于Class 2，实际上这里我们可以进一步认为x所对应的y是1或者0（因为这里假设只有两个类型），那么可以把式子进一步表示为：

![13](.\Resource\13.png)

这里使用了cross entropy（交叉熵）来表示了function的好坏，也就是说我们希望我们的模型结果和target（真实结果）更接近。这里就引出了Logistic Regression和Linear Regression的一个区别，那就是它们使用不同的Loss function，前者使用交叉熵，而后者使用平方误差（当然也有其他的方式）。下一步毫无疑问，依然是通过梯度下降法确定最好的参数，求Loss对$w$的偏导数可以得知（此处省去一大段求导计算）：


$$
w_i = w_i-\eta\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n
$$


为什么逻辑回归不适用平方误差作为Loss function呢，实际上如果我们尝试去这样做，在计算梯度下降的时候会发现在离target非常远的时候，会发现微分的结果也是0，这会导致在下降时速度非常慢甚至是无法下降，而Cross entropy就没有这个问题。

### 2.3 Discriminative和Generative

到这里就可以解答第一周的问题二了，之前我们选用高斯分布去进行分类的方法就是Generative，而直接找出$w$和$b$的方式则被称为Discriminative，它们假设的模型（Function set）看似是相同的，但是实际上它们会找出同样的参数么？答案是否定的。因为在Discriminative中我们没有假设任何的分布，而是直接去找出参数，在Generative中我们会进行一些分布上的假设，并通过分布的参数去计算最后模型的参数。一般来说Discriminative model会比Generative model表现更好。但是这并不是绝对的，因为在一些情况下，生成模型对于判别模型是有优势的：

- 训练数据较少时，判别模型的表现受数据量影响较大，而生成模型受数据量影响较小。
- label有噪声时。生成模型的假设（“脑补”）反而可以把数据中的问题忽视掉。
- 判别模型直接求后验概率，而生成模型将后验概率拆成先验和似然，而先验和似然可能来自不同来源。（例如语音识别中，说出一句话的几率是不需要大量的训练数据去决定的）

### 2.4 多分类的问题

之前为了举例简单，我们的分类都是两个类的情况。如果遇到了多分类问题，我们需要把sigmoid函数换成softmax函数，这很好理解，因为当多于两个类的时候，我们不再能直接用0或者1来表示不同的类了，如何去给这些类赋值呢？实际上多个类的输出可能是差距较大的，而softmax的目的就是放大这种差距：

![14](.\Resource\14.png)

而在softmax过程结束后就可以去跟target去计算交叉熵，并以最小化交叉熵为目的进行参数选择

### 2.5 逻辑回归的限制

实际上逻辑回归会有很明显的限制，例如feature的值可能导致无法分类，对于这种线性不可分的数据，需要考虑在逻辑回归建模之前进行Feature transformation（特征变化），举例说明：

| x1   | x2   | Label  |
| ---- | ---- | ------ |
| 0    | 0    | Class2 |
| 0    | 1    | Class1 |
| 1    | 0    | Class1 |
| 1    | 1    | Class2 |

这里如果直接要将这四个点进行逻辑回归，会发现它们无法分类，而我们可以使用特征迁移的方法进行处理，假设有另外两个点$x_1'[0,0]$和$x_2'[1,1]$，对这四个input的点求到这两个点的距离，可以得到每个input的两个新特征，表示为：

![15](.\Resource\15.png)

这里选用的特征迁移方式毕竟简单，实际上在应对具体问题的时候找到一个合适的特征迁移方式有时候非常困难，如何去寻找适当的特征迁移方案呢？我们可以考虑将多个逻辑回归模型串联，对输入首先进行特征迁移，之后再添加最后的判断（分类）的逻辑回归模型。而如果我们观察这个结构就会发现，某个逻辑回归的input是其他逻辑回归的output，这同样构成了神经网络。

回到第一周的问题3，为什么要在这里增加深度呢？其实和线性回归中增加模型深度的理由很相似，增加深度的过程本身就是为模型带来更多非线性元素，从而提高模型的复杂度，而更复杂的模型意味着更有可能模拟真实情况。同样的，作为特征迁移过程，本身结合实际问题也可能需要复杂的过程，而深度（神经网络）就提供了这种可能性。而如果采取增加特征广度的方式进行升维处理，理论上来说也有可能使得模型变为线性可分，但是这需要结合具体问题，例如模型是否还可以引入新的特征，或者说引入之后会不会仍然是无法摆脱这个limitation的。

## 三、对模型的优化和错误处理

本节对应着课程第二大周的内容，章节主题是“what to do if my network fails to train”

### 3.1 机器学习基本任务攻略

在本周的前置课程中老师介绍了如果在作业中遇到结果并不满意的时候，有一个基本的指导攻略：

![16](.\Resource\16.png)

首先我们需要从训练集上表现出的loss入手，如loss较大的话，说明它在训练集上就没有得到很好的学习。这件事有两种可能，第一是model bias问题，这个问题出现的原因是模型过于简单，即就算讨论了所有的函数集，它们构成的范围也太小了，因此找不到一个特别好的模型使得loss很小（可以认为模型结构下最好的也不够好）。此时我们需要更改模型，加入更多的feature或者是增加层数以加入更多的非线性元素。但是loss较大并不是只有这一个理由，还有一种可能性是模型的优化过程不够好，导致无法找到可能的范围内最好的那个模型。这里的两种情况就好像是大海捞针，前一种是最好的针可以捞到但是本身不够优秀，而第二种是捞不到那根最好的针。

这里就引出了一个问题，那就是如何去达到一个比较好的优化效果呢？给出的建议是从较浅的网络开始运行，或者甚至不用deep learning的方法，这样比较容易做optimize过程，会在能力范围之内找到最好的模型。这种前置过程会让我们有一个概念，即简单的模型会做到一个什么水平，如果我们之后引入深度模型，发现得到的loss还不如简单模型，就会知道应该是在optimize过程出现了问题，因为深度神经网络会比简单的模型更有弹性，如果发挥出最好的结果应该是毫无疑问优于简单模型的。另外更强力的优化方式在后面的课程会介绍到。

进一步来说，假设我们经过努力使得训练集上的loss较小了，那么我们就可以在测试集上进行检验，如果loss也较小，那么训练过程就完美结束了。但是如果此时在测试集上的loss较大，那么这里我们可能就遇到了overfitting的问题，即模型复杂度过高，在训练集上的拟合程度非常高，但是由于训练集并没有代表所有的数据，所以反而在校验集带来了更多的loss（注意，只有当training data上的loss小，testing data的loss大才称为**过拟合**）。此时我们可以考虑收集更多的数据，或者进行**Data augmentation**（数据增强），例如在图像识别的时候进行左右翻转等，根据合理的方式进行合理数据的制造。当然增加数据的方式在课程作业中是不允许的。另外一种解决过拟合的方式是给模型多一些的限制，例如我们根据先验知识知道模型可能的样子大概是怎么样的，就可以提前加以限制。具体的方式是给它数量少一些的参数或者是共用某些参数，又或者加入更少的feature。

这里还提到另外一个优化的方式是，将训练集分出一些作为validation set，使用cross validation（在validation set中先验证并选出loss最小的模型）或者是n-ford方法（将training set切成多等分，并选取不同的部分作为validation set并选出几次测试后平均loss最小的模型）

最后一个问题是mismatch，这是一种特殊的情况，指的是训练集和测试集的分布是不一样的，例如训练集是2021年的数据，而测试集是2022年的数据，那么毫无疑问会导致在测试集上的loss较大，这个问题无法使用添加更多的数据集来解决，具体的解决方案会放到作业11中讲述。

### 3.2 局部最小值和鞍点

根据梯度下降的原理，我们知道当gradient接近零的时候，模型就不会再继续优化了，loss的变化也会卡在某个水平，这里我们遇到的问题可能是local minima（局部最小值）或者saddle point（鞍点），它们的共同点就是gradient为0，出现这种情况的点称为critical point。那么我们如何知道出现问题时是哪种情况呢？

 ![17](.\Resource\17.png)

上图显示的是在点$\theta'$附近的其他点的Loss如何计算，如果我们在一个critical point处分析，那么第二项和gradient有关的部分就一定是0，那么判断具体的情况就取决于后面的Hessian matrix：

![18](.\Resource\18.png)

这里判断的方式也很清晰，假设$(\theta-\theta')$这两个向量的差为一个向量$v$，那么这里我们可以得知：

- 对所有的$v$来说，$v^THv>0$，则说明在$\theta'$附近所有的$L(\theta)>L(\theta')$，说明是Local minima
- 对所有的$v$来说，$v^THv<0$，则说明在$\theta'$附近所有的$L(\theta)<L(\theta')$，说明是Local maxima
- 对所有的$v$来说，$v^THv$的结果有时大于0有时小于0，则说明在$\theta'$附近的$L(\theta)$大小是不确定的，说明是Saddle point

这里好像我们要去看所有的$v$是否都满足某一条件，但是实际上我们只需要去看矩阵$H$是否是正定矩阵，也就是说计算矩阵$H$的所有本征值，如果本征值都大于0则说明它就是Local minima的情况，其他两种的判断方式也相似。（本征值：[矩阵特征值_百度百科 (baidu.com)](https://baike.baidu.com/item/矩阵特征值/8309765)）

实际上，如果出现的情况是鞍点，那么Hessian matrix还可以告诉我们参数可能的更新方向，我们只需要找出一个负的eigen value，并得出对应的某个eigen vector，当$(\theta-\theta')$等于eigen vector（沿着某一方向）的时候就可以使得Loss变小了。实际上在操作时，我们往往会发现Eigen value通常都是有正有负的，很少有Network是真的可以走到Local minima的。

### 3.3 批次和动量

在第一周的课程中我们了解到在训练过程中，往往会将数据集分为多个**批次（batch）**，这些批次会分开去各进行一次梯度下降过程，而所有批次都进行过一次则称为一个**循环（epoch）**，每一个循环之后会进行一次shuffle，即改变分割batch的方式。

如果我们考虑一个简单的情况，对于只有20个数据的训练资料，如果我们设置batch size为20，这种情况称为Full batch，在这种情况下模型需要看完所有的数据才会进行一个更新，而如果我们设置batch size为1，即每看一个数据就进行一次更新。对于前者来说，训练周期较长，但是训练的更新效果会很明显，而后者的训练周期很短，但是每一步走的并不稳定。它们各自有各自的优缺点，但是需要注意的是，batch size大并不一定代表训练的时间就更长，因为存在平行计算的技术，size在1到1000的范围内实际上计算时间并没有很多差距，但是再大就会有明显的区别了（使用Tesla V100 GPU的测试数据）。而实际上来说在一个循环内，batch size较小的会带来更多的时间，这是因为会花费更多的时间去读入数据（一个一个读）。因此，使用时间来作为二者的对比实际上是不太准确的。

![19](.\Resource\19.png)

以上是两个经典的机器学习任务中不同的batch size下的准确度，包括了训练集上的和校验集上的，我们会发现当batch size过大的时候训练效果会打折扣。为什么小一些的batch size会表现更好呢？我们可以想象一个情况，在梯度下降时，可能会在某一处卡住，如果我们使用的是full batch这种极端情况，那么我们是没有直接的方法去解决的，但是如果我们分为了多个batch，那么不同batch会生成的模型实际上是有轻微的区别的，可能就在某一处可以让我们在另一个batch上解决卡住的问题。最后，两者的差别总结在此，batch size也是我们需要考虑的一个hyperparameter：

![20](.\Resource\20.png)

之后来讲**动量（momentum）**这个概念，他的原理是梯度递减的时候不止去计算梯度，并且结合上一步的变化，具体如下所示：

![21](.\Resource\21.png)

引入动量有哪些好处呢？首先如果我们遇到了local minima的情况，由于我们在不断结合之前的动作，可能进一步的去跨越gradient为0的情况，而如果之后还可能走到一个更低的loss的地方。

### 3.4 自动调整学习速率

学习速率（Learning Rate）是梯度下降过程中的另一个非常关键的参数，我们可以想象，如果我们的learning rate设置的过大，就算梯度很大，我们也无法到达那个最低点，反而是在两个较大的点来回跳跃，就好像是在峡谷的两端来回而无法到达谷底，因为我们固定了它的步长，导致其无法进行很小的移动。

那么如何去选择一个合适的learning rate呢，当梯度很小的时候，我们可能希望学习率更大一些，而在梯度很大的时候，我们会希望学习率更小一些。所以我们需要一个客制化（定制化）的学习率，以应对梯度的变化，这里我们就需要引入一个新的参数$\sigma$去对学习率做一个adaptive的效果，这个参数可以用gradient的Root Mean Square来计算：

![22](.\Resource\22.png)

但是这个版本虽然能达到根据gradient的大小做一个制衡，但还是不能满足我们所有的要求，因为在多个参数加持下的Error Surface可能是非常复杂的，因此我们会用到另一个更adaptive的要求，称为RMSProp，在刚才提到的方法称为Adagrad，它每个gradient的权重是相同的，而RMS方法在计算时会引入新的参数$\alpha$从而自定义每个gradient的权重，它的表达是：


$$
\sigma^t_i=\sqrt{\alpha(\sigma^{t-1}_i)^2+(1-\alpha)(g^t_i)^2}
$$


这里可以理解为最近的gradient会有更大的权重，而更久远的gradient有更小的权重。现在一个很常用的优化策略（optimizer）就是Adam，它就是RMSProp加上Momentum。到现在为止，如果我们去实际测试一下，会发现到后期接近最低loss的时候会出现极大的抖动和步长，这是因为刚开始在y轴走了很远，转弯后虽然gradient小，但是积累了很多小的$\sigma$，导致步伐变大，就会突然到gradient大的地方，然后又因为gradient大，步伐变小，就又慢慢回到中间。因此我们需要引入另外一个方法，即Learning Rate Decay，让$\eta$和时间相关，当时间增加的时候，让学习率降低，这是以为随着时间变长，我们离目标越来越近，就需要减少学习率让参数更新慢下来。

最后课程介绍一个“黑科技”，即warm up方法，它的思路是根据时间先将学习率变大然后变小，大小需要自己去调整，为什么要这么做呢？这个问题至今仍是值得研究的，但是以恶合理的解释是，$\sigma$一开始的结果是统计得来的，需要大量的数据才会精准，因此我们一开始不应该走太远，而要慢慢搜集情报。

## 四、图像输入

本节对应课程第三周的课程（image as input）

### 4.1 卷积神经网络

Convolutional Neural Network（CNN）译为卷积神经网络，在探讨图象识别问题时，我们需要了解它的概念。在图像识别过程中，输入是一个固定像素点数量的图片，而输出是一个辨识的类别。对于模型来说，图片会作为一个三维的tensor输入，一个维度是图片的宽，一个维度是图片的高，而一个维度是图片的三个channel（RGB三个颜色），例如一个100x100的图片就会作为一个3x100x100的向量输入。可以理解为这个向量里每一维的数值是某一个像素点某一个颜色的强度。但是实际操作的时候，我们需要所有的像素点信息么？这样可能会导致模型参数过多，可能发生过拟合现象。

假设我们需要识别一个图片是不是鸟，或许我们可以去在中间的神经元判断是否识别到了鸟嘴、鸟眼、鸟爪等，而这些信息是不需要整个图片作为输入来识别的。在CNN中，我们会设置一个Receptive field（感受野），它是整个图片的一小部分，每个神经元都只关注它的感受野，（多个神经元可以有同样的receptive field，而且它们之间可以重叠）但是如何去定义我们的Receptive field呢？

这里我们首先了解一些基本概念，对于一个Receptive field来说，它的大小称为kernel size（例如3x3），通常我们不会将其设置太大。实际上，一个Receptive field通常会有很多神经元在“监视”。我们可以通过移动感受野去识别整个图像，而整个移动的步长称为stride，注意我们通常希望感受野彼此之间有高度的重叠，这是为了避免特征在间隔处出现，而如果我们发现超出了影像的范围，我们可能还需要去补值（例如补上0），示意图如下所示：

![23](.\Resource\23.png)

但是有个问题是，特征可能出现在图片的任何地方，例如图片中鸟嘴可能出现在左上角，也可能出现在中间，但是我们应该去让每个范围都覆盖到可以识别到鸟嘴的神经元么？这样毫无疑问会导致参数过多。我们可以使用一种共用参数的方法，即对于不同的Receptive Field使用同样的weight去构造神经元，例如对于上面的例子，每个field有64个神经元在检测，我们就将这一批固定的参数作为每一个field的神经元结构，这里的每一个参数就称为filter。

总结一下，为了对模型加以限制，我们首先使用了Receptive Field的概念，进一步的，又使用Parameter Sharing方法进一步限制参数。而这两者结合的方式就构成了Convolutional Layer。（如果这种方式不好理解，我们也可以理解为有一组filter去检测图片的小范围并扫过整张图片）实际上卷积层的加持可能会导致更大的model bias，但是由于其是对于影像识别设计的，在该问题上仍能达到较好的效果。

再举一个数字识别的例子，不同的数字如果切分到小的部分，会有一些特征，例如8和9会有半圆弧，1和7的直线等等，而filter可以理解为发现这些小特征的过程，每个filter对应不同的小特征，而每个filter去扫过整个图片去识别是否能检测到。如果存在几个特征的组合（例如有半弧和直线），它就更偏向于去选择组合可能的数字。（例如9）

### 4.2 为什么使用深度学习

该部分为第三周中延续之前第二周中讨论的一个矛盾，即在做optimization的时候，如果可选择的参数很多，那么我们可以降低理想状态的Loss，但是也会使得理想和现实有较大差距（在训练集中的表现和在所有数据中的表现差距较大），而如果我们使用较少的可选参数，可能会使得理想和现实差距较小，但是也会造成模型本身的Loss就较大。因此如何去寻找一个鱼与熊掌兼得的方法呢？也就是说，我们需要用较少的参数的同时，去保证Loss也较小。

之前我们已经谈到了为何去引入深度神经网络，它的目的是为模型带来非线性的属性，使其可以模拟更复杂的问题模型，每个hidden layer中提供了不同的piecewise linear，从而提供任何类型的function。但是为何我们需要引入更多的层呢？（因为实际上如果我们有一个更fat的网络，也可以做到逼近更复杂的模型）

实际上对于这个问题，使用深度的结构可以更有效率的模拟实际函数，即使用更少的参数（对比在一层加入过多的参数）这里可以结合电路设计去理解，如果我们用两层的逻辑电路就可以构建任何类型的解决方案，但是它需要的元件数量是巨大的，如果我们使用更多的层数，就可以使用更少的元件去完成任务。再举个例子，在写程序的时候，我们也会避免将所有的function都写在同一个类里，以避免同一个类过于冗长。这里也用到了深层的概念。

这里给出一个简单的例子：

![24](.\Resource\24.png)

这里是一个简单的hidden layer，下方的图展示了a2和x的关系，那么我们再添加一层：

![25](.\Resource\25.png)

这样就得出了a3和x的关系，是一个8段的锯齿，但是如果我们用一层（shallow）的方式去模拟这个函数，会需要2的K次方的神经元（对应2的K次方的线段S）

### 4.3 Spatial Transformer

这里介绍一个特殊的架构，即在4.1中提到过的CNN还不足以应对放大缩小或旋转的问题（这在数据增强的时候非常常见，即创造新的图片作为训练数据），因此介绍Spatial Transformer layer，这个layer的目的是对图片做旋转和缩放，将图片中重要的部分“提取”出来，而它本身也是一个神经网络，可以理解为在原来的CNN前面多叠了一个Layer，这里给出一个例子：

![26](.\Resource\26.png)

所以从原理上来说它如何做到这一点呢？无论是旋转、平移还是缩放，本质上是某一个像素点移动到另一个像素点，如果我们知道起点和终点，我们只需要把它们之间的weight置为1，而起点到其他像素点的weight置为0，就可以在layer中完成这个操作，但是问题是如何去找这些对应关系呢？（具体的操作如下所示：）

![27](.\Resource\27.png)

这样操作的问题在于，可能无法进行gradient decent的过程，也就无法训练network了，因为可以想象如果我们给的值不那么整，很可能的出来的结果也不是整数，我们只能近似的归到最近的地方，例如a(2,2)如果保持位置不变，它的gradient可能就是0，所以我们需要进行interpolation过程：

![28](.\Resource\28.png)

这个步骤使得参数的细微变化可以被察觉，它的gradient也就再是0了，就可以进行gradient decent过程了。另外，在具体操作过程中，我们甚至可以多次进行ST过程，或者是在同一层进行两个ST过程。

## 五、序列输入

该部分对应课程第4周的内容，即Sequence as input

### 5.1 自注意力机制

在之前的问题中，我们模型的输入都是一个vector，但是如果我们的问题更复杂，例如假设我们的输入是有多个vector，甚至它们的长度和数量会改变，那么要怎么办呢？这种场景实际上并不少见，例如在文字处理方面，句子的长度和每个词的长度都是不一样的，对应着就是vector的长度和数量是不一样多的。又例如在语音识别中，我们通常会设置一个window，它框选出来的是一小段的声音讯号，而移动window就会扫描整个声音讯号，这样的话随着window选出的段落不一样，也会产生很多的vector作为输出。

那么对于这种问题来说，它的输出会是怎样的呢？这里有三种情况：

- 输出和vector数量一样多的结果（例如词性识别中一个句子的每个词都要标记，语音识别中每一段音频都要转化为文字）
- 输出为一个结果（例如对句子做情绪识别，又或者是语音识别中识别出是谁在讲话）
- 输出为不确定量的结果（在这种任务中是由模型本身决定输出的数量的，例如AI翻译，由于语言的不同可能输入输出的数量也是不一样的。这种任务也叫sequence to sequence）

在这一小节我们先关注第一种情况。其实如果简单来想，既然输入和输出的数量一致，我们是不是可以直接把输入当作多次的输入来处理呢？这种思路有点像fully connected的网络，但是实际上这并不合理，例如如果我们要做词性识别，对于“I saw a saw”这个短句来说，两个saw的词性是不一样的，但是如果只是简单的对每个vector去套用模型，并不会直接输出不同的结果。因此很显然我们需要考虑更多的信息，貌似使用一个window去把每个vector的neighbour也考虑进来放到网络中当成输入，可以有一定的效果，但是实际上这种做法还是有局限，因为window不能无限大，而过大的window也会影响时间。因此我们需要介绍Self-attention，即自注意力机制，它的作用就是考虑到所有的输入vector并输出同数量的输出。

对于Self-attention，第一步是找出sequence中和当前处理的vector相关的vector，进一步来说，我们需要找出每个vector和当前vector的关联度，设置为$\alpha$，计算的方式如下所示：

![29](.\Resource\29.png)

实际上，a1也要和自己计算一次关联性，这里的一些操作的矩阵例如$W^q$和$W^k$在这里的课程还没有介绍具体如何去获取，暂时先认为是一组特殊的矩阵。在计算过后，根据这些关联性分数再进行一次normalization例如SoftMax，然后分别乘上每个vector的结果，这样谁的关联性更大，谁的结果就会有更大的占比。图示如下：

![30](.\Resource\30.png)

这样得知了如何去得到下一层的b1，而其他的b也是可以一样的得到的（实际上是同时被计算出来的），总的来看，每个a都需要进行三种运算，分别是计算q，k和v，假设输入的一批向量结合起来看成一个矩阵$I$，而输出的一批向量同样的看成三种矩阵$Q, K, V$，那就可以如下的表示：


$$
q^i=W^qa^i \qquad Q=W^qI \\
k^i=W^ka^i \qquad K=W^kI \\
v^i=W^va^i \qquad V=W^vI
$$


每一个q都会和去每一个k相乘得到关联度，这个过程可以统一描述为如下所示：

![](.\Resource\31.png)

之后我们会对这个$A'$进行与V相乘的操作得到B的结果（这里全用大写代表由所有结果组成的矩阵）。但是如果我们的input之间不止有一种关系（多个head的情况，称为Multi-head Self-attention），我们可能会对于每个input都需要多个q，多个k和多个v，执行上的方式仍然是和单个关系相似的，当然了这样也会得到多个b的结果，需要再经过一个矩阵得到最终的b。

刚才的方案有一个致命的缺陷，就是没有考虑到vector之间的位置资讯，但是这个信息实际上是很关键的，例如在词性识别的时候我们会根据词在句子中的位置去判定它的词性。因此我们会了解到一个称为Positional Encoding的方法，即为每个不同位置的vector给到一个独特的位置vector，称为$e^i$，而如何这个vector如何产生有很多方法，目前还没有讨论出哪种是最好的方式。

Self-attention的应用实际上非常广泛，不管是语音的识别还是图像的识别都可以用到它，但是在使用时我们会去人为的设定一些vector的输入方式，例如在语音中去切段避免A过大，或者在图像中去根据RGB分为三层等。在图像识别中，实际上self-attention可以看作是一种更高级的CNN，带有自主学习性质的reception field，弹性较好。